"""
Unified Reasoning Engine

Performs chain-of-thought reasoning in a single LLM call for:
- Context understanding
- Intent detection
- Agent routing
- Response guidance
- Memory updates
"""

import json
import logging
import re
import time
from typing import Optional, Dict, Any, List
from pydantic import BaseModel, Field

from patient_ai_service.core.llm import LLMClient, get_llm_client
from patient_ai_service.core.config import settings
from patient_ai_service.core.observability import get_observability_logger
from patient_ai_service.core.conversation_memory import (
    ConversationMemoryManager,
    get_conversation_memory_manager
)
from patient_ai_service.core import get_state_manager
from patient_ai_service.models.validation import (
    ValidationResult,
    ExecutionLog,
    ToolExecution
)
from patient_ai_service.models.observability import TokenUsage

logger = logging.getLogger(__name__)


class UnderstandingResult(BaseModel):
    """What the reasoning engine understood about the user's message."""
    what_user_means: str
    is_continuation: bool = False
    continuation_type: Optional[str] = None  # "selection", "confirmation", "rejection", "modification", "clarification"
    selected_option: Optional[Any] = None  # The option user selected
    sentiment: str = "neutral"  # "affirmative", "negative", "neutral", "unclear"
    is_conversation_restart: bool = False


class RoutingResult(BaseModel):
    """Where to route the message."""
    agent: str  # "registration", "appointment_manager", etc.
    action: str
    urgency: str = "routine"  # "routine", "urgent", "emergency"


class MemoryUpdate(BaseModel):
    """Updates to apply to conversation memory."""
    new_facts: Dict[str, Any] = Field(default_factory=dict)
    system_action: str = ""
    awaiting: str = ""


class TaskContext(BaseModel):
    """
    Structured context for agent execution.
    """
    user_intent: str = ""
    entities: Dict[str, Any] = Field(default_factory=dict)
    success_criteria: List[str] = Field(default_factory=list)
    constraints: List[str] = Field(default_factory=list)
    prior_context: Optional[str] = None
    
    # Continuation fields
    is_continuation: bool = False
    continuation_type: Optional[str] = None  # "selection", "confirmation", "clarification"
    selected_option: Optional[Any] = None  # The option user selected
    continuation_context: Optional[Dict[str, Any]] = Field(default=None)

class ResponseGuidance(BaseModel):
    """Guidance for the selected agent's response."""
    tone: str = "helpful"  # "helpful", "empathetic", "urgent", "professional"
    task_context: TaskContext = Field(default_factory=TaskContext)
    minimal_context: Dict[str, Any] = Field(default_factory=dict)
    plan: str = ""  # Short step-by-step plan/command for the agent to follow (deprecated)


class ReasoningOutput(BaseModel):
    """Complete output from unified reasoning engine."""
    understanding: UnderstandingResult
    routing: RoutingResult
    memory_updates: MemoryUpdate
    response_guidance: ResponseGuidance
    reasoning_chain: List[str] = Field(default_factory=list)


# =============================================================================
# CONTINUATION DETECTION
# =============================================================================

class ContinuationDetector:
    """
    Detects when a user message is a continuation/response to previous options.
    """
    
    # Affirmative responses
    AFFIRMATIVE_PATTERNS = [
        r"^(yes|yeah|yep|yup|sure|ok|okay|alright|sounds good|perfect|great|fine)\.?$",
        r"^(that works|that\'s fine|that\'s good|go ahead|please do|do it)\.?$",
        r"^(the first one|the second one|the third one|first|second|third)\.?$",
        r"^(option [123a-c]|[123a-c])\.?$"
    ]
    
    # Time selection patterns
    TIME_PATTERNS = [
        r"^(\d{1,2})(:\d{2})?\s*(am|pm)?\.?$",  # "3", "3pm", "3:00 pm"
        r"^(the )?\d{1,2}(:\d{2})?\s*(am|pm)?( one)?\.?$",  # "the 3pm one"
    ]
    
    # Negative responses
    NEGATIVE_PATTERNS = [
        r"^(no|nope|nah|not really|neither|none)\.?$",
        r"^(actually|wait|hold on|never ?mind)\.?",
    ]
    
    @classmethod
    def detect_continuation_type(
        cls,
        message: str,
        awaiting: Optional[str] = None,
        presented_options: Optional[List[Any]] = None
    ) -> Dict[str, Any]:
        """
        Detect if message is a continuation and what type.
        
        Returns:
            {
                "is_continuation": True/False,
                "continuation_type": "selection" | "confirmation" | "rejection" | "modification",
                "selected_option": The option user selected (if applicable),
                "confidence": 0.0-1.0
            }
        """
        message_lower = message.lower().strip()
        
        result = {
            "is_continuation": False,
            "continuation_type": None,
            "selected_option": None,
            "confidence": 0.0
        }
        
        # Check for affirmative response
        for pattern in cls.AFFIRMATIVE_PATTERNS:
            if re.match(pattern, message_lower, re.IGNORECASE):
                result["is_continuation"] = True
                result["continuation_type"] = "confirmation"
                result["confidence"] = 0.9
                
                # Check for ordinal selection
                if "first" in message_lower and presented_options:
                    result["continuation_type"] = "selection"
                    result["selected_option"] = presented_options[0] if presented_options else None
                elif "second" in message_lower and presented_options and len(presented_options) > 1:
                    result["continuation_type"] = "selection"
                    result["selected_option"] = presented_options[1]
                elif "third" in message_lower and presented_options and len(presented_options) > 2:
                    result["continuation_type"] = "selection"
                    result["selected_option"] = presented_options[2]
                
                return result
        
        # Check for time selection
        for pattern in cls.TIME_PATTERNS:
            match = re.match(pattern, message_lower, re.IGNORECASE)
            if match:
                result["is_continuation"] = True
                result["continuation_type"] = "selection"
                result["confidence"] = 0.85
                
                # Extract the time value
                time_value = cls._extract_time(message_lower)
                result["selected_option"] = time_value
                
                # Validate against presented options if available
                if presented_options:
                    matched = cls._match_time_to_options(time_value, presented_options)
                    if matched:
                        result["selected_option"] = matched
                        result["confidence"] = 0.95
                
                return result
        
        # Check for negative response
        for pattern in cls.NEGATIVE_PATTERNS:
            if re.match(pattern, message_lower, re.IGNORECASE):
                result["is_continuation"] = True
                result["continuation_type"] = "rejection"
                result["confidence"] = 0.85
                return result
        
        # Check if it matches one of the presented options directly
        if presented_options:
            for option in presented_options:
                option_str = str(option).lower()
                if option_str in message_lower or message_lower in option_str:
                    result["is_continuation"] = True
                    result["continuation_type"] = "selection"
                    result["selected_option"] = option
                    result["confidence"] = 0.95
                    return result
        
        # Check based on what we're awaiting
        if awaiting:
            if awaiting == "time_selection" and cls._looks_like_time(message_lower):
                result["is_continuation"] = True
                result["continuation_type"] = "selection"
                result["selected_option"] = cls._extract_time(message_lower)
                result["confidence"] = 0.8
                return result
            
            elif awaiting == "confirmation" and len(message_lower) < 20:
                # Short message when awaiting confirmation is likely a response
                result["is_continuation"] = True
                result["continuation_type"] = "clarification"
                result["confidence"] = 0.6
                return result
        
        return result
    
    @classmethod
    def _extract_time(cls, message: str) -> str:
        """Extract time value from message."""
        # Look for time patterns
        match = re.search(r'(\d{1,2})(?::(\d{2}))?\s*(am|pm)?', message, re.IGNORECASE)
        if match:
            hour = match.group(1)
            minute = match.group(2) or "00"
            period = match.group(3)
            
            if period:
                return f"{hour}:{minute} {period}"
            return f"{hour}:{minute}"
        
        return message
    
    @classmethod
    def _looks_like_time(cls, message: str) -> bool:
        """Check if message looks like a time."""
        return bool(re.search(r'\d{1,2}(:\d{2})?\s*(am|pm)?', message, re.IGNORECASE))
    
    @classmethod
    def _match_time_to_options(cls, time_value: str, options: List[Any]) -> Optional[Any]:
        """Try to match a time value to one of the presented options."""
        # Normalize the time
        time_lower = time_value.lower().replace(" ", "")
        
        for option in options:
            option_str = str(option).lower().replace(" ", "")
            
            # Direct match
            if time_lower == option_str:
                return option
            
            # Extract hour and check
            time_match = re.search(r'(\d{1,2})', time_lower)
            option_match = re.search(r'(\d{1,2})', option_str)
            
            if time_match and option_match:
                if time_match.group(1) == option_match.group(1):
                    # Same hour - likely a match
                    return option
        
        return None


class ReasoningEngine:
    """
    Unified chain-of-thought reasoning engine.

    Performs comprehensive reasoning in a single LLM call:
    - Understands user message in context
    - Determines routing and urgency
    - Extracts new facts
    - Provides response guidance
    """

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        memory_manager: Optional[ConversationMemoryManager] = None,
        test_mode: bool = False
    ):
        """
        Initialize reasoning engine.

        Args:
            llm_client: LLM client for reasoning (defaults to GPT-4o mini or Claude Haiku)
            memory_manager: Conversation memory manager
            test_mode: Enable deterministic test mode
        """
        self.llm_client = llm_client or get_llm_client()
        self.memory_manager = memory_manager or get_conversation_memory_manager()
        self.state_manager = get_state_manager()
        self.test_mode = test_mode

        # For deterministic testing
        self._test_responses: Dict[str, ReasoningOutput] = {}

        logger.info(f"Initialized ReasoningEngine (test_mode={test_mode})")

    def set_test_response(self, message: str, response: ReasoningOutput):
        """
        Set a deterministic response for testing.

        Args:
            message: User message to match
            response: Predefined reasoning output to return
        """
        self._test_responses[message] = response
        logger.debug(f"Set test response for message: {message}")

    async def reason(
        self,
        session_id: str,
        user_message: str,
        patient_info: Dict[str, Any] = None
    ) -> ReasoningOutput:
        """
        Perform unified reasoning about the user's message.

        Single LLM call that performs:
        - Context analysis
        - Intent detection
        - Agent routing
        - Memory updates
        - Response guidance

        Args:
            session_id: Session identifier
            user_message: User's message
            patient_info: Patient information dict

        Returns:
            Complete reasoning output
        """
        # TEST MODE: Return mocked response
        if self.test_mode and user_message in self._test_responses:
            logger.info(f"Returning test response for: {user_message}")
            return self._test_responses[user_message]

        # Get conversation memory
        memory = self.memory_manager.get_memory(session_id)

        # Get continuation context from state manager
        continuation_context = None
        try:
            continuation_context = self.state_manager.get_continuation_context(session_id)
        except Exception as e:
            logger.debug(f"Could not get continuation context: {e}")
            continuation_context = None

        # Pre-detect continuation to help LLM
        continuation_detection = None
        if continuation_context:
            continuation_detection = ContinuationDetector.detect_continuation_type(
                user_message,
                awaiting=continuation_context.get("awaiting"),
                presented_options=continuation_context.get("presented_options")
            )
            
            if continuation_detection.get("is_continuation"):
                logger.info(
                    f"Pre-detected continuation: type={continuation_detection.get('continuation_type')}, "
                    f"selected={continuation_detection.get('selected_option')}, "
                    f"confidence={continuation_detection.get('confidence')}"
                )

        # Check for conversation restart
        if self.memory_manager.is_conversation_restart(session_id, user_message):
            logger.info(f"Conversation restart detected for session {session_id}")
            return ReasoningOutput(
                understanding=UnderstandingResult(
                    what_user_means="Starting new conversation",
                    is_conversation_restart=True,
                    sentiment="neutral"
                ),
                routing=RoutingResult(
                    agent="general_assistant",
                    action="greet_user",
                    urgency="routine"
                ),
                memory_updates=MemoryUpdate(),
                response_guidance=ResponseGuidance(
                    tone="helpful",
                    minimal_context={}
                ),
                reasoning_chain=["Conversation restart detected", "Route to general assistant"]
            )

        # Build unified reasoning prompt with continuation context
        prompt = self._build_reasoning_prompt(
            user_message, 
            memory, 
            patient_info or {},
            continuation_context=continuation_context
        )
        logger.info(">" * 80)
        logger.info(f"ðŸ§  Reasoning Prompt: {prompt}")
        logger.info(">" * 80)

        
        # Get observability logger
        obs_logger = get_observability_logger(session_id) if settings.enable_observability else None
        reasoning_tracker = obs_logger.reasoning_tracker if obs_logger else None

        try:
            # Record reasoning step
            if reasoning_tracker:
                reasoning_tracker.record_step(1, "Building reasoning prompt", {
                    "user_message": user_message[:100],
                    "memory_summary": memory.summary[:200] if memory.summary else None
                })
                # Broadcast reasoning step (fire and forget)
                if obs_logger and obs_logger._broadcaster:
                    try:
                        import asyncio
                        step_data = {
                            "step_number": 1,
                            "description": "Building reasoning prompt",
                            "context": {
                                "user_message": user_message[:100],
                                "memory_summary": memory.summary[:200] if memory.summary else None
                            }
                        }
                        try:
                            loop = asyncio.get_running_loop()
                            # Schedule task without awaiting
                            loop.create_task(obs_logger._broadcaster.broadcast_reasoning_step(step_data))
                        except RuntimeError:
                            # No running loop, create new one
                            asyncio.run(obs_logger._broadcaster.broadcast_reasoning_step(step_data))
                    except Exception as e:
                        logger.debug(f"Error broadcasting reasoning step: {e}")
            
            # Single LLM call does everything
            llm_start_time = time.time()
            reasoning_temp = settings.reasoning_temperature
            if hasattr(self.llm_client, 'create_message_with_usage'):
                response, tokens = self.llm_client.create_message_with_usage(
                    system=self._get_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=reasoning_temp
                )
            else:
                response = self.llm_client.create_message(
                    system=self._get_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=reasoning_temp
                )
                tokens = TokenUsage()

            llm_duration_ms = (time.time() - llm_start_time) * 1000

            # Record LLM call
            if obs_logger:
                llm_call = obs_logger.record_llm_call(
                    component="reasoning",
                    provider=settings.llm_provider.value,
                    model=settings.get_llm_model(),
                    tokens=tokens,
                    duration_ms=llm_duration_ms,
                    system_prompt_length=len(self._get_system_prompt()),
                    messages_count=1,
                    temperature=reasoning_temp,
                    max_tokens=settings.llm_max_tokens
                )
            
            # Record reasoning step
            if reasoning_tracker:
                reasoning_tracker.record_step(2, "LLM reasoning call completed", {
                    "response_length": len(response),
                    "tokens_used": tokens.total_tokens
                })

            # Parse and validate response with continuation awareness
            output = self._parse_reasoning_response(
                response, 
                user_message, 
                memory,
                continuation_context=continuation_context,
                continuation_detection=continuation_detection
            )

            # [CRITICAL] Inject language context into minimal_context
            # This ensures ALL agents receive language awareness
            global_state = self.state_manager.get_global_state(session_id)
            language_context = global_state.language_context

            # Add language context to minimal_context
            output.response_guidance.minimal_context["current_language"] = language_context.current_language
            output.response_guidance.minimal_context["current_dialect"] = language_context.current_dialect

            logger.info(
                f"Injected language context into minimal_context: "
                f"{language_context.get_full_language_code()}"
            )

            # Record reasoning details
            if reasoning_tracker:
                reasoning_tracker.set_understanding({
                    "what_user_means": output.understanding.what_user_means,
                    "is_continuation": output.understanding.is_continuation,
                    "continuation_type": output.understanding.continuation_type,
                    "selected_option": output.understanding.selected_option,
                    "sentiment": output.understanding.sentiment,
                    "is_conversation_restart": output.understanding.is_conversation_restart
                })
                reasoning_tracker.set_routing({
                    "agent": output.routing.agent,
                    "action": output.routing.action,
                    "urgency": output.routing.urgency
                })
                reasoning_tracker.set_memory_updates({
                    "new_facts": output.memory_updates.new_facts,
                    "system_action": output.memory_updates.system_action,
                    "awaiting": output.memory_updates.awaiting
                })
                reasoning_tracker.set_response_guidance({
                    "tone": output.response_guidance.tone,
                    "minimal_context": output.response_guidance.minimal_context
                })
                
                # Record reasoning chain steps
                for i, step_desc in enumerate(output.reasoning_chain, start=3):
                    reasoning_tracker.record_step(i, step_desc, {})
                
                # Set LLM call in reasoning details
                reasoning_details = reasoning_tracker.get_details()
                if obs_logger and llm_call:
                    reasoning_details.llm_call = llm_call
                    obs_logger.reasoning_tracker = reasoning_tracker

            # Update memory with extracted facts
            if output.memory_updates.new_facts:
                self.memory_manager.update_facts(session_id, output.memory_updates.new_facts)

            # Update system state
            if output.memory_updates.system_action or output.memory_updates.awaiting:
                self.memory_manager.update_system_state(
                    session_id,
                    last_action=output.memory_updates.system_action or None,
                    awaiting=output.memory_updates.awaiting or None
                )

            # Check if summarization needed
            if self.memory_manager.should_summarize(session_id):
                logger.info(f"Triggering summarization for session {session_id}")
                await self.memory_manager.summarize(session_id)

            logger.info(f"Reasoning complete for session {session_id}: "
                       f"agent={output.routing.agent}, "
                       f"urgency={output.routing.urgency}, "
                       f"sentiment={output.understanding.sentiment}")

            # Print full reasoning output to terminal logs
            logger.info("=" * 80)
            logger.info("REASONING OUTPUT (Full Structure):")
            logger.info("=" * 80)
            logger.info(f"UNDERSTANDING:")
            logger.info(f"  - what_user_means: {output.understanding.what_user_means}")
            logger.info(f"  - is_continuation: {output.understanding.is_continuation}")
            logger.info(f"  - sentiment: {output.understanding.sentiment}")
            logger.info(f"  - is_conversation_restart: {output.understanding.is_conversation_restart}")
            logger.info(f"ROUTING:")
            logger.info(f"  - agent: {output.routing.agent}")
            logger.info(f"  - action: {output.routing.action}")
            logger.info(f"  - urgency: {output.routing.urgency}")
            logger.info(f"MEMORY_UPDATES:")
            logger.info(f"  - new_facts: {json.dumps(output.memory_updates.new_facts, indent=4) if output.memory_updates.new_facts else '(empty)'}")
            logger.info(f"  - system_action: '{output.memory_updates.system_action}' {'(EMPTY - should be filled!)' if not output.memory_updates.system_action else ''}")
            logger.info(f"  - awaiting: '{output.memory_updates.awaiting}' {'(EMPTY - OK if nothing needed)' if not output.memory_updates.awaiting else ''}")
            logger.info(f"RESPONSE_GUIDANCE:")
            logger.info(f"  - tone: {output.response_guidance.tone}")
            logger.info(f"  - minimal_context: {json.dumps(output.response_guidance.minimal_context, indent=4)}")
            logger.info(f"  - task_context.user_intent: {output.response_guidance.task_context.user_intent}")
            logger.info(f"  - task_context.entities: {json.dumps(output.response_guidance.task_context.entities, indent=4)}")
            logger.info(f"  - task_context.success_criteria: {output.response_guidance.task_context.success_criteria}")
            logger.info(f"  - task_context.is_continuation: {output.response_guidance.task_context.is_continuation}")
            logger.info(f"  - plan: {output.response_guidance.plan}")
            logger.info(f"REASONING_CHAIN:")
            for i, step in enumerate(output.reasoning_chain, 1):
                logger.info(f"  {i}. {step}")
            logger.info("=" * 80)

            return output

        except Exception as e:
            logger.error(f"Error in reasoning engine: {e}", exc_info=True)
            return self._fallback_reasoning(user_message, memory, patient_info or {})

    def _get_system_prompt(self) -> str:
        """Get system prompt for reasoning engine."""
        return """You are the reasoning engine for a dental clinic AI assistant.

Your job is to THINK through each user message and understand:
1. What the user is really saying (in context)
2. What they actually need
3. How best to help them

You analyze conversations holistically, reading between the lines to provide
natural, context-aware assistance.

IMPORTANT PRINCIPLES:
- Short responses like "yeah", "ok", "sure" are usually responses to what the system said
- If system proposed something and user agrees, honor that
- Don't force users to repeat themselves or use specific keywords
- Consider emotional state - frustrated users need empathy
- Emergency situations take priority (severe pain, bleeding, knocked out teeth)
- The goal is to HELP, not to categorize

Always respond with your reasoning in the specified JSON format."""

    def _build_reasoning_prompt(
        self,
        user_message: str,
        memory: Any,
        patient_info: Dict[str, Any],
        continuation_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Build the unified reasoning prompt with continuation awareness."""

        # Format recent turns
        recent_turns_formatted = "\n".join([
            f"{'User' if turn.role == 'user' else 'Assistant'}: {turn.content}"
            for turn in memory.recent_turns
        ])

        # Continuation context section
        continuation_section = ""
        if continuation_context:
            continuation_section = f"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONTINUATION CONTEXT (Previous flow was interrupted)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

The system is waiting for: {continuation_context.get('awaiting', 'user response')}
Options presented to user: {json.dumps(continuation_context.get('presented_options', []), indent=2)}
Original request: {continuation_context.get('original_request', 'Unknown')}
Resolved so far: {json.dumps(continuation_context.get('resolved_entities', {}), indent=2)}

IMPORTANT: Check if user's message is a response to the above!
"""

        return f"""Analyze this conversation situation and respond with ONE complete JSON.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONVERSATION STATE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**User Facts (persistent, never lost):**
{json.dumps(memory.user_facts, indent=2) if memory.user_facts else "None collected yet"}

**Conversation Summary:**
{memory.summary if memory.summary else "No previous conversation"}

**Recent Messages (last {len(memory.recent_turns)} turns):**
{recent_turns_formatted if recent_turns_formatted else "No previous messages"}

**System State:**
- Last Action: {memory.last_action or "None"}
- Awaiting Response: {memory.awaiting or "Nothing specific"}
{continuation_section}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
PATIENT INFORMATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

- Name: {patient_info.get('first_name', 'Unknown')}
- Registered: {'Yes' if patient_info.get('patient_id') else 'No - NOT registered yet'}
- Patient ID: {patient_info.get('patient_id', 'None')}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NEW USER MESSAGE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"{user_message}"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
CONTINUATION DETECTION RULES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

If user's message is short (1-3 words) AND system was awaiting a response:
- "yeah", "ok", "sure" â†’ User CONFIRMS previous suggestion
- "3pm", "4pm", "2:30" â†’ User SELECTS a time from options
- "the first one" â†’ User selects first option
- "no", "neither" â†’ User REJECTS options, needs alternatives
- "actually..." â†’ User wants to CHANGE something

When detected as continuation:
- Set is_continuation: true
- Set continuation_type: "selection" | "confirmation" | "rejection"
- Extract selected_option if applicable
- Include resolved_entities from continuation context

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
YOUR TASK
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Analyze and respond with ONLY a valid JSON object.

CRITICAL RULES:
- Output ONLY the JSON object, no other text
- Do NOT include comments (no // or # characters)
- Use lowercase true/false for booleans (not True/False)
- Use null for missing values (not None or "null")

PLAN GENERATION (for response_guidance.plan):
- ALWAYS generate a plan when routing to appointment_manager
- Plan should be 2-4 concise steps
- Include specific tool names and actions
- Examples:
  * "1. Check if patient is registered (if not, redirect to registration), 2. Get doctor list, 3. Check availability for requested date/time, 4. Book appointment if available"
  * "1. Get patient's existing appointments, 2. Display appointment details"
  * "1. Get appointment by ID, 2. Update status to cancelled with reason"
- For other agents, plan can be brief or empty

JSON STRUCTURE:

{{
    "understanding": {{
        "what_user_means": "Plain English explanation of what user actually wants",
        "is_continuation": true/false,  # Is this continuing previous topic?
        "continuation_type": "selection/confirmation/rejection/new_request/null",
        "selected_option": "value user selected or null",
        "sentiment": "affirmative/negative/neutral/unclear",
        "is_conversation_restart": false
    }},
    "routing": {{
        "agent": "appointment_manager/registration/general_assistant/medical_inquiry/emergency_response",
        "action": "Specific action for agent to take",
        "urgency": "routine/urgent/emergency"
    }},
    "memory_updates": {{
        "new_facts": {{}},  # NEW facts only (don't repeat existing user_facts)
        "system_action": "What the system/agent DID so far in this conversation. Examples: 'asked_for_date', 'provided_doctor_list', 'checked_availability', 'proposed_registration', 'showed_appointments', 'asked_for_confirmation'. Use past tense. REQUIRED - always provide this.",
        "awaiting": "What the system is waiting for from the user. Examples: 'date_selection', 'time_confirmation', 'doctor_choice', 'user_info', 'confirmation', 'appointment_id'. Use empty string '' if not waiting for anything. REQUIRED - always provide this (even if empty)."
    }},
    "response_guidance": {{
        "tone": "helpful/empathetic/urgent/professional",
        "task_context": {{
            "user_intent": "What user wants (incorporate continuation context if resuming)",
            "success_criteria": [
                // Same criteria as before if resuming blocked flow
            ],
            "constraints": [],
            "prior_context": "Relevant context including previous options",
            "is_continuation": true/false,
            "continuation_type": "selection/confirmation/rejection/null",
            "selected_option": "The option user selected",
            "continuation_context": {{
                // Copy from continuation_context if resuming
            }}
        }},
        "minimal_context": {{
            "user_wants": "Brief what user wants",
            "action": "Suggested action",
            "prior_context": "Any relevant prior context"
        }},
        "plan": "Short step-by-step plan for the agent."
    }},
    "reasoning_chain": [
        "Step 1: What I observe...",
        "Step 2: What I conclude...",
        "Step 3: Therefore..."
    ]
}}

KEY RULES:
1. If system proposed action (check "Last Action") and user said "yeah/ok/sure/yes" â†’ is_continuation=true, sentiment=affirmative
2. Emergency keywords (severe bleeding, can't breathe, knocked out, severe pain) â†’ urgency=emergency
3. Extract ONLY NEW facts to new_facts (don't repeat what's in user_facts already)
4. For minimal_context, keep it BRIEF - just essential info
5. If user not registered and wants to book appointment â†’ recommend registration first
6. Short ambiguous responses like "tomorrow" â†’ likely continuation of previous topic

RESPOND WITH VALID JSON ONLY - NO OTHER TEXT."""

    def _parse_reasoning_response(
        self,
        response: str,
        user_message: str,
        memory: Any,
        continuation_context: Optional[Dict[str, Any]] = None,
        continuation_detection: Optional[Dict[str, Any]] = None
    ) -> ReasoningOutput:
        """Parse LLM response into structured reasoning output with continuation awareness."""

        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise ValueError("No JSON found in response")

            json_str = json_match.group()

            # Try direct parse first
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.warning(f"Initial JSON parse failed: {e}")
                
                # Apply repairs
                repaired = json_str
                
                # Remove // comments
                repaired = re.sub(r'//[^\n]*', '', repaired)
                
                # Remove # comments  
                repaired = re.sub(r'#[^\n]*', '', repaired)
                
                # Fix Python-style booleans/None
                repaired = re.sub(r'\bTrue\b', 'true', repaired)
                repaired = re.sub(r'\bFalse\b', 'false', repaired)
                repaired = re.sub(r'\bNone\b', 'null', repaired)
                
                # Fix trailing commas
                repaired = re.sub(r',(\s*[}\]])', r'\1', repaired)
                
                # Fix missing commas
                repaired = re.sub(r'"\s*\n\s*"', '",\n"', repaired)
                repaired = re.sub(r'(\}})\s*\n\s*"', r'}},\n"', repaired)
                repaired = re.sub(r'(\])\s*\n\s*"', r'],\n"', repaired)
                
                try:
                    data = json.loads(repaired)
                    logger.info("Successfully repaired JSON")
                except json.JSONDecodeError as e2:
                    logger.error(f"JSON repair failed: {e2}")
                    # Use fallback instead of raising
                    logger.warning(f"Using fallback reasoning for: {user_message[:50]}")
                    return self._fallback_reasoning(user_message, memory, {})

            # Parse nested structures
            understanding_data = data.get("understanding", {})
            routing_data = data.get("routing", {})
            memory_data = data.get("memory_updates", {})
            guidance_data = data.get("response_guidance", {}) or {}
            task_context_data = guidance_data.get("task_context", {}) or {}
            reasoning_chain = data.get("reasoning_chain", [])

            # Enhance with continuation detection if LLM missed it
            is_continuation = understanding_data.get("is_continuation", False)
            continuation_type = understanding_data.get("continuation_type")
            selected_option = understanding_data.get("selected_option")

            # Use pre-detection if LLM didn't detect continuation
            if continuation_detection and continuation_detection.get("is_continuation"):
                if not is_continuation or continuation_detection.get("confidence", 0) > 0.8:
                    is_continuation = True
                    if not continuation_type:
                        continuation_type = continuation_detection.get("continuation_type")
                    if not selected_option:
                        selected_option = continuation_detection.get("selected_option")
                    logger.info(f"Using pre-detected continuation: {continuation_type}")

            understanding_data["is_continuation"] = is_continuation
            understanding_data["continuation_type"] = continuation_type
            understanding_data["selected_option"] = selected_option

            understanding = UnderstandingResult(**understanding_data)
            routing = RoutingResult(**routing_data)
            memory_updates = MemoryUpdate(**memory_data)

            # Handle task_context - merge resolved entities from continuation context
            if continuation_context and is_continuation:
                resolved_entities = continuation_context.get("resolved_entities", {})
                if resolved_entities:
                    # Merge resolved entities into task_context entities
                    if "entities" not in task_context_data:
                        task_context_data["entities"] = {}
                    task_context_data["entities"].update(resolved_entities)
                    logger.info(f"ðŸ”„ [ReasoningEngine] Merged {len(resolved_entities)} resolved entities from continuation: {json.dumps(resolved_entities, default=str)}")

                # Add selected option to entities if applicable
                if selected_option and continuation_type == "selection":
                    if "entities" not in task_context_data:
                        task_context_data["entities"] = {}
                    # Add selected option based on what was awaited
                    awaiting = continuation_context.get("awaiting", "")
                    if "time" in awaiting.lower():
                        task_context_data["entities"]["selected_time"] = selected_option
                    elif "doctor" in awaiting.lower():
                        task_context_data["entities"]["selected_doctor"] = selected_option
                    else:
                        task_context_data["entities"]["selected_option"] = selected_option

            # Ensure task_context is created
            if not task_context_data:
                task_context_data = {}

            # Clean None values that should be empty dicts/lists
            # (LLM may return null instead of omitting the field)
            if task_context_data.get("continuation_context") is None:
                task_context_data["continuation_context"] = {}
            if task_context_data.get("entities") is None:
                task_context_data["entities"] = {}
            if task_context_data.get("success_criteria") is None:
                task_context_data["success_criteria"] = []
            if task_context_data.get("constraints") is None:
                task_context_data["constraints"] = []

            # Create TaskContext
            task_context = TaskContext(**task_context_data)

            # Log entities extracted by reasoning engine
            entities = task_context_data.get("entities", {})
            if entities:
                logger.info(f"ðŸ“Š [ReasoningEngine] Extracted entities from user message: {json.dumps(entities, default=str)}")
            else:
                logger.info(f"ðŸ“Š [ReasoningEngine] No entities extracted from user message")

            # Create ResponseGuidance from guidance_data
            # Extract fields from guidance_data with safe defaults
            tone = guidance_data.get("tone", "helpful")
            minimal_context = guidance_data.get("minimal_context", {})
            plan = guidance_data.get("plan", "")
            
            # Create ResponseGuidance object
            response_guidance = ResponseGuidance(
                tone=tone,
                task_context=task_context,
                minimal_context=minimal_context,
                plan=plan
            )

            # Create output
            output = ReasoningOutput(
                understanding=understanding,
                routing=routing,
                memory_updates=memory_updates,
                response_guidance=response_guidance,
                reasoning_chain=reasoning_chain
            )

            # Special case: affirmative response to registration proposal
            if (understanding.is_continuation and
                understanding.sentiment == "affirmative" and
                "registration" in memory.last_action.lower()):
                output.routing.agent = "registration"
                logger.info("Detected affirmative response to registration proposal")

            return output

        except Exception as e:
            logger.error(f"Error parsing reasoning response: {e}")
            logger.debug(f"Response was: {response}")
            raise

    def _fallback_reasoning(
        self,
        user_message: str,
        memory: Any,
        patient_info: Dict[str, Any]
    ) -> ReasoningOutput:
        """Fallback reasoning when LLM fails."""

        message_lower = user_message.lower().strip()

        # Check for emergency keywords
        emergency_keywords = [
            "emergency", "urgent", "severe bleeding", "can't breathe",
            "knocked out", "broken jaw", "severe pain", "911", "ambulance"
        ]
        is_emergency = any(kw in message_lower for kw in emergency_keywords)

        if is_emergency:
            return ReasoningOutput(
                understanding=UnderstandingResult(
                    what_user_means="User has a dental emergency",
                    is_continuation=False,
                    sentiment="urgent",
                    is_conversation_restart=False
                ),
                routing=RoutingResult(
                    agent="emergency_response",
                    action="assess_and_respond",
                    urgency="emergency"
                ),
                memory_updates=MemoryUpdate(
                    system_action="responding_to_emergency",
                    awaiting=""
                ),
                response_guidance=ResponseGuidance(
                    tone="urgent",
                    minimal_context={"user_wants": "emergency help", "action": "assess_emergency"}
                ),
                reasoning_chain=["Emergency keywords detected", "Route to emergency response"]
            )

        # Check for affirmative responses to proposals
        affirmatives = ["yes", "yeah", "yep", "sure", "okay", "ok", "alright", "please", "go ahead"]
        is_affirmative = any(aff in message_lower for aff in affirmatives)

        if is_affirmative and "registration" in memory.last_action.lower():
            return ReasoningOutput(
                understanding=UnderstandingResult(
                    what_user_means="Agrees to register",
                    is_continuation=True,
                    sentiment="affirmative",
                    is_conversation_restart=False
                ),
                routing=RoutingResult(
                    agent="registration",
                    action="start_registration",
                    urgency="routine"
                ),
                memory_updates=MemoryUpdate(
                    system_action="starting_registration",
                    awaiting="user_info"
                ),
                response_guidance=ResponseGuidance(
                    tone="helpful",
                    minimal_context={"user_wants": "register", "action": "collect_info"}
                ),
                reasoning_chain=["User affirmed", "Last action was registration proposal", "Start registration"]
            )

        # Default fallback
        logger.warning(f"Using default fallback reasoning for: {user_message}")
        return ReasoningOutput(
            understanding=UnderstandingResult(
                what_user_means=user_message,
                is_continuation=False,
                sentiment="neutral",
                is_conversation_restart=False
            ),
            routing=RoutingResult(
                agent="general_assistant",
                action="understand_and_respond",
                urgency="routine"
            ),
            memory_updates=MemoryUpdate(),
            response_guidance=ResponseGuidance(
                tone="helpful",
                minimal_context={}
            ),
            reasoning_chain=["Fallback reasoning used"]
        )

    async def validate_response(
        self,
        session_id: str,
        original_reasoning: ReasoningOutput,
        agent_response: str,
        execution_log: ExecutionLog
    ) -> ValidationResult:
        """
        Validate agent response against original intent and execution reality.

        This closes the loop by checking:
        1. Did agent complete the expected task?
        2. Does response match tool results?
        3. Is agent being honest about what happened?
        4. Any safety/policy violations?

        Args:
            session_id: Session identifier
            original_reasoning: The original reasoning output that routed to this agent
            agent_response: The response the agent generated
            execution_log: Log of tools executed and their results

        Returns:
            ValidationResult with decision on whether to send, retry, or fallback
        """
        try:
            # Build validation prompt
            prompt = self._build_validation_prompt(
                original_reasoning,
                agent_response,
                execution_log
            )

            # Call LLM for validation (low temperature for consistency)
            obs_logger = get_observability_logger(session_id) if settings.enable_observability else None
            llm_start_time = time.time()

            validation_temp = settings.validation_temperature
            if hasattr(self.llm_client, 'create_message_with_usage'):
                response, tokens = self.llm_client.create_message_with_usage(
                    system=self._get_validation_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=validation_temp
                )
            else:
                response = self.llm_client.create_message(
                    system=self._get_validation_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=validation_temp
                )
                tokens = TokenUsage()
            
            llm_duration_ms = (time.time() - llm_start_time) * 1000
            
            # Record LLM call
            llm_call = None
            if obs_logger:
                llm_call = obs_logger.record_llm_call(
                    component="validation",
                    provider=settings.llm_provider.value,
                    model=settings.get_llm_model(),
                    tokens=tokens,
                    duration_ms=llm_duration_ms,
                    system_prompt_length=len(self._get_validation_system_prompt()),
                    messages_count=1,
                    temperature=0.2,
                    max_tokens=settings.llm_max_tokens
                )

            # Parse validation response
            validation = self._parse_validation_response(response)
            
            # Add LLM call to validation details
            if obs_logger:
                from patient_ai_service.models.observability import ValidationDetails
                validation_details = ValidationDetails(
                    is_valid=validation.is_valid,
                    confidence=validation.confidence,
                    decision=validation.decision,
                    issues=validation.issues,
                    reasoning=validation.reasoning,
                    feedback_to_agent=validation.feedback_to_agent,
                    llm_call=llm_call,
                    retry_count=0
                )
                obs_logger.set_validation_details(validation_details)

            logger.info(f"Validation complete for session {session_id}: "
                       f"valid={validation.is_valid}, "
                       f"decision={validation.decision}, "
                       f"confidence={validation.confidence}")

            return validation

        except Exception as e:
            logger.error(f"Error in validation: {e}", exc_info=True)
            # On validation error, assume response is valid (fail open)
            return ValidationResult(
                is_valid=True,
                confidence=0.5,
                decision="send",
                issues=[f"Validation error: {str(e)}"],
                reasoning=["Validation failed, defaulting to send"]
            )

    async def finalize_response(
        self,
        session_id: str,
        original_reasoning: ReasoningOutput,
        agent_response: str,
        execution_log: ExecutionLog,
        validation_result: ValidationResult
    ) -> ValidationResult:
        """
        Finalize agent response by approving or editing before sending to user.

        This is the SECOND layer of quality control that runs AFTER validation (and retry if needed).
        Even if validation passed, finalization ensures response is grounded in tool results.

        The reasoning engine examines:
        1. Original user intent (from reasoning output)
        2. Actual tool execution results (from execution log)
        3. Agent's response accuracy
        4. Previous validation feedback (if retry occurred)

        Three outcomes:
        - APPROVE: Response is accurate and complete, send as-is
        - EDIT: Minor issues or improvements needed, provide edited version
        - FALLBACK: Cannot confidently approve, escalate to human

        Args:
            session_id: Session identifier
            original_reasoning: Initial reasoning before agent execution
            agent_response: The response from agent (possibly after retry)
            execution_log: Record of all tool calls and results
            validation_result: Result from validation layer (for context)

        Returns:
            ValidationResult with optional rewritten_response
        """
        try:
            # Build finalization prompt
            prompt = self._build_finalization_prompt(
                original_reasoning,
                agent_response,
                execution_log,
                validation_result
            )

            # Call LLM for finalization (slightly higher temp for natural edits)
            obs_logger = get_observability_logger(session_id) if settings.enable_observability else None
            llm_start_time = time.time()

            finalization_temp = settings.finalization_temperature
            if hasattr(self.llm_client, 'create_message_with_usage'):
                response, tokens = self.llm_client.create_message_with_usage(
                    system=self._get_finalization_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=finalization_temp
                )
            else:
                response = self.llm_client.create_message(
                    system=self._get_finalization_system_prompt(),
                    messages=[{"role": "user", "content": prompt}],
                    temperature=finalization_temp
                )
                tokens = TokenUsage()
            
            llm_duration_ms = (time.time() - llm_start_time) * 1000
            
            # Record LLM call
            llm_call = None
            if obs_logger:
                llm_call = obs_logger.record_llm_call(
                    component="finalization",
                    provider=settings.llm_provider.value,
                    model=settings.get_llm_model(),
                    tokens=tokens,
                    duration_ms=llm_duration_ms,
                    system_prompt_length=len(self._get_finalization_system_prompt()),
                    messages_count=1,
                    temperature=finalization_temp,
                    max_tokens=settings.llm_max_tokens
                )

            # Parse finalization response
            finalization = self._parse_finalization_response(response)
            
            # Add LLM call to finalization details
            if obs_logger:
                from patient_ai_service.models.observability import FinalizationDetails
                finalization_details = FinalizationDetails(
                    decision=finalization.decision,
                    confidence=finalization.confidence,
                    was_rewritten=finalization.was_rewritten,
                    rewritten_response_preview=finalization.rewritten_response[:200] if finalization.rewritten_response else "",
                    issues=finalization.issues,
                    reasoning=finalization.reasoning,
                    llm_call=llm_call
                )
                obs_logger.set_finalization_details(finalization_details)

            logger.info(f"Finalization complete for session {session_id}: "
                       f"decision={finalization.decision}, "
                       f"edited={finalization.was_rewritten}, "
                       f"confidence={finalization.confidence}")

            return finalization

        except Exception as e:
            logger.error(f"Error in finalization: {e}", exc_info=True)
            # On finalization error, approve agent response (fail open)
            return ValidationResult(
                is_valid=True,
                confidence=0.5,
                decision="send",
                issues=[f"Finalization error: {str(e)}"],
                reasoning=["Finalization failed, defaulting to agent response"]
            )

    def _get_finalization_system_prompt(self) -> str:
        """System prompt for response finalization (second-layer quality check)."""
        return """You are a response finalizer for a dental clinic AI system.

CONTEXT:
This is the FINAL quality check before sending response to user. The response has already been:
1. Validated for major issues
2. Potentially retried if validation failed
Now you perform a final check to approve or make minor edits.

YOUR TASK:
Review the agent's response and ensure it:
1. Accurately reflects tool execution results
2. Answers the user's actual request
3. Is grounded in facts (no hallucinations)
4. Is complete and helpful

CRITICAL CHECKS:
- If agent says "appointment confirmed" â†’ verify book_appointment was called with success=true
- If agent provides data (dates, times, doctor names, confirmation numbers) â†’ verify exact match with tool outputs
- If agent says "no availability" â†’ verify check_availability returned available=false
- Agent should NEVER invent information not present in tool results

DECISION OUTCOMES:
1. "send" - Response is accurate and complete, send as-is (rewritten_response = null, is_valid = true)
2. "edit" - Minor issues detected, provide edited version (rewritten_response = edited text, is_valid = false, decision = "edit")
3. "fallback" - Cannot confidently approve, escalate to human (decision = "fallback")

RESPONSE FORMAT (JSON):
{
    "is_valid": true/false,
    "confidence": 0.0-1.0,
    "decision": "send|edit|fallback",
    "issues": ["issue 1", "issue 2"],
    "reasoning": ["reasoning step 1", "step 2"],
    "rewritten_response": "edited response text" or null,
    "was_rewritten": true/false
}

GROUNDING PRINCIPLE:
All responses must be grounded in actual tool outputs. If tools say available=true,
response cannot say "no availability". If book_appointment returns "APT-123",
response MUST include "APT-123".

WHEN EDITING:
- Maintain the agent's conversational tone
- Fix only inaccuracies or missing information
- Include all relevant information from tool outputs
- Keep it natural and helpful
- Make minimal changes (don't rewrite unnecessarily)

FAIL OPEN:
If you're uncertain or detect a complex issue, use decision="fallback" to escalate to human."""

    def _build_finalization_prompt(
        self,
        original_reasoning: ReasoningOutput,
        agent_response: str,
        execution_log: ExecutionLog,
        validation_result: ValidationResult
    ) -> str:
        """Build prompt for response finalization (second-layer quality check)."""
        import json

        # Format tool executions for context
        tools_section = ""
        if execution_log.tools_used:
            tools_section = "\n\nTOOL EXECUTIONS:\n"
            for i, tool in enumerate(execution_log.tools_used, 1):
                tools_section += f"\n{i}. {tool.tool_name}"
                tools_section += f"\n   Inputs: {json.dumps(tool.inputs, indent=2)}"
                tools_section += f"\n   Outputs: {json.dumps(tool.outputs, indent=2)}"
        else:
            tools_section = "\n\nTOOL EXECUTIONS: None (no tools were called)"

        # Include validation context if retry occurred
        validation_context = ""
        if not validation_result.is_valid:
            validation_context = f"""

VALIDATION HISTORY:
The response was validated and retried. Previous validation found these issues:
{chr(10).join(f"- {issue}" for issue in validation_result.issues)}

The agent was given this feedback and retried:
{validation_result.feedback_to_agent}

The current response is the result AFTER retry."""

        prompt = f"""ORIGINAL USER INTENT:
{original_reasoning.understanding.what_user_means}

ROUTING DECISION:
Agent: {original_reasoning.routing.agent}
Action: {original_reasoning.routing.action}
{tools_section}
{validation_context}

AGENT'S RESPONSE (final version after validation/retry):
{agent_response}

TASK:
Perform final quality check on the agent's response.
- If response is accurate and complete â†’ approve (decision: "send")
- If minor issues detected â†’ provide edited version (decision: "edit")
- If you cannot confidently approve â†’ escalate (decision: "fallback")

Respond in JSON format as specified in the system prompt."""

        return prompt

    def _parse_finalization_response(self, response: str) -> ValidationResult:
        """Parse LLM finalization response into ValidationResult."""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise ValueError("No JSON found in finalization response")

            import json
            data = json.loads(json_match.group())

            # Create ValidationResult with finalization fields
            return ValidationResult(
                is_valid=data.get("is_valid", True),
                confidence=data.get("confidence", 1.0),
                decision=data.get("decision", "send"),
                issues=data.get("issues", []),
                reasoning=data.get("reasoning", []),
                feedback_to_agent=data.get("feedback_to_agent", ""),
                rewritten_response=data.get("rewritten_response"),
                was_rewritten=data.get("was_rewritten", False)
            )

        except Exception as e:
            logger.error(f"Error parsing finalization response: {e}")
            logger.debug(f"Response was: {response}")
            # On parse error, approve agent response (fail open)
            return ValidationResult(
                is_valid=True,
                confidence=0.5,
                decision="send",
                issues=[f"Parse error: {str(e)}"],
                reasoning=["Failed to parse finalization, approving agent response"]
            )

    def _get_validation_system_prompt(self) -> str:
        """Get system prompt for validation LLM call."""
        return """You are a validation system for a dental clinic AI assistant.

Your job is to verify that agent responses are:
1. COMPLETE - Did the agent finish the task?
2. ACCURATE - Does the response match what actually happened?
3. HONEST - Is the agent claiming something that didn't occur?
4. SAFE - No policy violations or dangerous advice?

CRITICAL VALIDATION CHECKS:

Tool Usage:
- If task requires booking: Did agent call book_appointment tool?
- If agent says "booked": Is there a book_appointment result?
- If tool returned data: Did agent use it correctly?

Completeness:
- If user asked for confirmation number: Is it in the response?
- If user requested action: Was action completed?

Accuracy:
- Does response match tool results?
- Are dates/times valid?
- Do entity references exist?

Safety:
- No medical diagnosis
- No unauthorized actions
- No false confirmations

Always respond with structured JSON for automated processing."""

    def _build_validation_prompt(
        self,
        original_reasoning: ReasoningOutput,
        agent_response: str,
        execution_log: ExecutionLog
    ) -> str:
        """Build validation prompt with context."""
        tools_summary = "\n".join([
            f"- {exec.tool_name}: inputs={exec.inputs}, outputs={exec.outputs}"
            for exec in execution_log.tools_used
        ])

        return f"""VALIDATION REQUEST

â•â•â• ORIGINAL ANALYSIS â•â•â•
User wanted: {original_reasoning.understanding.what_user_means}
Expected action: {original_reasoning.routing.action}
Expected tools: (inferred from action type)

â•â•â• WHAT ACTUALLY HAPPENED â•â•â•
Agent response: "{agent_response}"

Tools executed:
{tools_summary if tools_summary else "No tools used"}

â•â•â• VALIDATION TASK â•â•â•
Check if response is valid and safe to send to user.

Respond with JSON:
{{
  "is_valid": true/false,
  "confidence": 0.0-1.0,
  "issues": ["list of specific problems found"],
  "decision": "send" | "retry" | "redirect" | "fallback",
  "feedback_to_agent": "specific guidance if retry needed",
  "reasoning": ["step 1: ...", "step 2: ..."]
}}

DECISION GUIDE:
- "send": Response is valid, send to user
- "retry": Fixable issue, give agent specific feedback
- "redirect": Wrong agent, need different approach
- "fallback": Unfixable, use safe fallback response

IMPORTANT: Only mark is_valid=true if you're confident response is complete, accurate, and safe."""

    def _parse_validation_response(self, response: str) -> ValidationResult:
        """Parse LLM validation response into ValidationResult."""
        try:
            # Extract JSON from response
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if not json_match:
                raise ValueError("No JSON found in validation response")

            data = json.loads(json_match.group())

            # Create ValidationResult
            return ValidationResult(
                is_valid=data.get("is_valid", False),
                confidence=data.get("confidence", 1.0),
                issues=data.get("issues", []),
                decision=data.get("decision", "send"),
                feedback_to_agent=data.get("feedback_to_agent", ""),
                reasoning=data.get("reasoning", [])
            )

        except Exception as e:
            logger.error(f"Error parsing validation response: {e}")
            logger.debug(f"Response was: {response}")
            # On parse error, assume valid (fail open)
            return ValidationResult(
                is_valid=True,
                confidence=0.5,
                decision="send",
                issues=[f"Parse error: {str(e)}"],
                reasoning=["Failed to parse validation, defaulting to send"]
            )


# Global instance
_reasoning_engine: Optional[ReasoningEngine] = None


def get_reasoning_engine() -> ReasoningEngine:
    """Get or create the global reasoning engine instance."""
    global _reasoning_engine
    if _reasoning_engine is None:
        _reasoning_engine = ReasoningEngine()
    return _reasoning_engine


def reset_reasoning_engine():
    """Reset the global reasoning engine (for testing)."""
    global _reasoning_engine
    _reasoning_engine = None